{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статья: \n",
    "https://habr.com/ru/articles/799725/\n",
    "- `Про бустинг в целом`: https://translated.turbopages.org/proxy_u/en-ru.ru.70266c94-681c3592-61f7101e-74722d776562/https/www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/\n",
    "\n",
    "- `Продолжение про бустинг`: https://translated.turbopages.org/proxy_u/en-ru.ru.70266c94-681c3592-61f7101e-74722d776562/https/www.geeksforgeeks.org/ml-gradient-boosting/\n",
    "\n",
    "## Ютуб:\n",
    "https://youtu.be/-cp0aVkoI5U?si=I010UW3YTxXhmdIx\n",
    "\n",
    "## Fridman_MSE\n",
    "https://machinelearning-basics.com/decision-trees-splitting-criteria-for-classification-and-regression/\n",
    "\n",
    "- это улучшанная версия MSE в Градиентном бустинге.\n",
    "\n",
    "`diff = yL - yR`\n",
    "\n",
    "`improvement = nL * nR * diff^(2) / nL  + nR` - улучшение качества разбиения\n",
    "\n",
    "- yL : среднее значение с левой стороны узла\n",
    "- yR : среднее значение в правой части узла .\n",
    "- nL: количество наблюдений в левой части узла.\n",
    "- nR: количество наблюдений в правой части узла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "class GBCustomRegressor:\n",
    "    \"\"\"\n",
    "    ОСНОВНАЯ ИДЕЯ: обучить модельки предсказывать антиградиент, чтобы лучше минимизировать ошибку\n",
    "\n",
    "    Условие:\n",
    "        - Для задачи регрессии лосс это квадратичная ошибка\n",
    "        - Нужно поддержать все параметры перечисленные в __init__\n",
    "        - Нужно написать реализацию всех перечисленных в теле класса методов\n",
    "        - В качестве реализации дерева решения нужно использовать DecisionTreeRegressor.\n",
    "        - Другими классами из sklearn пользоваться запрещается\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            criterion=\"friedman_mse\",\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_depth=3,\n",
    "            random_state=None\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate (float): Скорость обучения или подругому ШАГ, как в градиентном спуске. Defaults to 0.1.\n",
    "            n_estimators (int): Количество базовых алгоритмов. Defaults to 100.\n",
    "            criterion (str): отвечает за критерий сплита в дереве. Defaults to \"friedman_mse\".\n",
    "            min_samples_split (int):  Узел не должен делиться, если в нем объем выборке равен или меньше чем min samples split.  Defaults to 2.\n",
    "            min_samples_leaf (int): Именно в листьях должно быть не меньше чем min_sample_leaf выборок. Defaults to 1.\n",
    "            max_depth (int): Максимальная глубина дерева. Defaults to 3.\n",
    "            random_state (int): Начальная точка для рандомных чисел. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        self.learning_rate = learning_rate # использовал\n",
    "        self.n_estimators = n_estimators # использовал\n",
    "        self.criterion = criterion # использовал\n",
    "        self.min_samples_split = min_samples_split # использовал\n",
    "        self.min_samples_leaf = min_samples_leaf # использовал\n",
    "        self.max_depth = max_depth # использовал\n",
    "        self.random_state = random_state # использовал\\\n",
    "        self.trees = []\n",
    "\n",
    "    @staticmethod\n",
    "    def MSE_gradient(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        (-1) * grad(1/2(Y - alpha)^ 2 = y - alpha \n",
    "        \"\"\" \n",
    "        return y_true - y_pred\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Алгорит обучения: \n",
    "        \n",
    "        1) первоначальному прогнозу присваивается среднее значение y для всех образцов;\n",
    "\n",
    "        2) рассчитываются остатки модели на основе антиградиента функции потерь;\n",
    "\n",
    "        3) регрессионное дерево обучается на X и остатках, далее делается прогноз на X;\n",
    "\n",
    "        4) полученный прогноз добавляется к первоначальному и шаги 2-4 повторяются для каждого дерева;\n",
    "\n",
    "        5) после обучения всех моделей снова создаётся первоначальный прогноз из шага 1;\n",
    "\n",
    "        6) далее делаются прогнозы для X_test на обученных деревьях и добавляются к первоначальному;\n",
    "\n",
    "        7) полученная сумма и будет конечным прогнозом.\n",
    "\n",
    "        \"\"\"\n",
    "        self.base = np.mean(y)\n",
    "        self.y_pred = np.mean(y)\n",
    "        # У меня возник вопрос: почему мы инициализируем средним, в итоге понял из-за того что на начальном этапе - это самая оптимальная точка для минимизации фукции MSE, т.к. если взять производную от  MSE и приравнять ее к 0. Мы получим ymax = mean(y) <- вторую производную брать смысла нет, чтобы доказывать что это max...\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = self.MSE_gradient(y, self.y_pred) # это остатки на которых должна обучиться следующая модель\n",
    "            # был вопрос: почему мы тут берем антиградиент? Ответ: потому, что наши модельки пытаются предсказать - этот антиградиент, чтобы минимизировать ошибку \n",
    "            \n",
    "            temp_tree = DecisionTreeRegressor(criterion= self.criterion,\n",
    "                                              min_samples_split= self.min_samples_split,\n",
    "                                              min_samples_leaf= self.min_samples_leaf,\n",
    "                                              max_depth= self.max_depth,\n",
    "                                              random_state= self.random_state)\n",
    "            \n",
    "            temp_tree.fit(x, residuals)\n",
    "            \n",
    "            self.trees.append(temp_tree)\n",
    "\n",
    "            self.y_pred += self.learning_rate * temp_tree.predict(x) # вот здесь модель выдает градиент\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        summ = 0\n",
    "        for tree in self.trees:\n",
    "            tree:DecisionTreeRegressor\n",
    "            summ += tree.predict(x)\n",
    "        summ *= self.learning_rate\n",
    "\n",
    "        return self.base + summ\n",
    "\n",
    "    @property\n",
    "    def estimators_(self):\n",
    "        return self.trees \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GBCustomClassifier:\n",
    "    \"\"\"\n",
    "    ОСНОВНЫЕ ОТЛИЧИЯ от регрессионной модели:\n",
    "        - нужно преобразовывать y: onehotencoding (если много классов) / приводить к вероятностному виду используя сигмоиду\n",
    "        - использование logloss\n",
    "\n",
    "    Условие:\n",
    "        - Для бинарной классификации использовать logloss.\n",
    "        - Нужно поддержать все параметры перечисленные в __init__\n",
    "        - Нужно написать реализацию всех перечисленных в теле класса методов\n",
    "        - В качестве реализации дерева решения нужно использовать DecisionTreeRegressor.\n",
    "        - Другими классами из sklearn пользоваться запрещается\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            learning_rate=0.1, \n",
    "            n_estimators=100,\n",
    "            criterion=\"friedman_mse\",\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_depth=3,\n",
    "            random_state=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            learning_rate (float): Скорость обучения или подругому ШАГ, как в градиентном спуске. Defaults to 0.1.\n",
    "            n_estimators (int): Количество базовых алгоритмов. Defaults to 100.\n",
    "            criterion (str): отвечает за критерий сплита в дереве. Defaults to \"friedman_mse\".\n",
    "            min_samples_split (int):  Узел не должен делиться, если в нем объем выборке равен или меньше чем min samples split.  Defaults to 2.\n",
    "            min_samples_leaf (int): Именно в листьях должно быть не меньше чем min_sample_leaf выборок. Defaults to 1.\n",
    "            max_depth (int): Максимальная глубина дерева. Defaults to 3.\n",
    "            random_state (int): Начальная точка для рандомных чисел. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate # использовал\n",
    "        self.n_estimators = n_estimators # использовал\n",
    "        self.criterion = criterion # использовал\n",
    "        self.min_samples_split = min_samples_split # использовал\n",
    "        self.min_samples_leaf = min_samples_leaf # использовал\n",
    "        self.max_depth = max_depth # использовал\n",
    "        self.random_state = random_state # использовал\\\n",
    "        self.trees = []\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(y:np.array):\n",
    "        \"\"\"\n",
    "        Формула: 1 / 1 + exp(-x)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-y))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "\n",
    "        y_mean = np.mean(y)\n",
    "        self.base = np.log(y_mean / (1 - y_mean)) if y_mean not in (0, 1) else 0 # изменение от регрессии\n",
    "        y_pred = np.full_like(y, self.base, dtype= float)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            \n",
    "            p = self.sigmoid(y_pred) # изменение от регрессии регрессии\n",
    "            residuals = y - p\n",
    "            \n",
    "            temp_tree = DecisionTreeRegressor(min_samples_split= self.min_samples_split,\n",
    "                                              min_samples_leaf= self.min_samples_leaf,\n",
    "                                              max_depth= self. max_depth,\n",
    "                                              random_state= self.random_state,\n",
    "                                              criterion= self.criterion)\n",
    "            \n",
    "            temp_tree.fit(x, residuals, sample_weight= p * (1 - p))\n",
    "            \n",
    "            self.trees.append(temp_tree)\n",
    "            \n",
    "            y_pred += self.learning_rate * temp_tree.predict(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        logit = self.base + self.learning_rate * np.sum([tree.predict(x) for tree in self.trees])\n",
    "        return self.sigmoid(logit)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return (self.predict_proba(x) >= 0.5).astype(int)\n",
    "\n",
    "    @property\n",
    "    def estimators_(self):\n",
    "        return self.trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([6, 4, 5])\n",
    "np.vstack((a, b)).T[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
